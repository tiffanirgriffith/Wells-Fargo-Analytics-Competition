{"name":"Wells Fargo Analytics Competition","tagline":"","body":"### Competition Details\r\nWells Fargo invited College of Charleston along with 12 other universities to participation in a social media analytics competition. We were challenged to develop a repeatable process to understand the drivers from consumer financial conversations on twitter and Facebook.\r\nhttps://www.mindsumo.com/contests/wells-fargo \r\n\r\n### Our Methodology \r\nOur approach was to examine the data and isolate terms which could be used to distinguish the general feelings towards each individual banks’ foreign/abroad transactions. Our methodology was to isolate each bank and then run a sentiment analysis for positive and negative words (provided by positive and negative lexicons) within posts containing the words foreign and abroad. We ran a sentiment analysis and interpreted the graphs and ratios. Below is a description and visual representation of our analytic process flow. \r\n\r\n### Analytic Process Flow\r\nOur code reflects our analytical process flow in that we extracted the raw data and manually reviewed some banks for common topic. We then used R to clean the data. We removed punctuation, non-important words, numbers, etc. We also divided the data into bank specific data frames. We then ran sentiment analysis for negative and positive feelings towards foreign affairs and abroad transactions for each bank. We analyzed our data and tried to make inferences based on the negative to positive ratios. \r\nWe noticed many topics; some keywords we found were withdrawn, overdrawn, customer service, and foreign/abroad transactions. We focused on the topics of foreign affairs and abroad transactions. The substances were determined for each bank based on their negative to positive ratios. Below are our graphs for the positive and negative sentiment analysis for each bank.\r\nhttp://i.imgur.com/9IKTuGv.png\r\n\r\n### Our Code \r\n\r\n```R\r\nCleaning up the data:\r\nLoading R:\r\ndf = read.table('Dataset.txt',sep=\"|\",header=T)\r\ndf$FullText = as.character(df$FullText)\r\n# Grab just the texts, so you can load them in the Corpus\r\ndf.texts = as.data.frame(df[,ncol(df)])\r\ncolnames(df.texts) = 'FullText'\r\n# Remove non-ascii characters\r\ndf.texts.clean = as.data.frame(iconv(df.texts$FullText, \"latin1\", \"ASCII\", sub=\"\"))\r\ncolnames(df.texts.clean) = 'FullText'\r\ndf$FullText = df.texts.clean$FullText\r\n# If you want to test on just 1000 records using df.1000 created below\r\nidx.1000 = sample(1:nrow(df),1000)\r\ndf.1000 = df[idx.1000,]\r\n# Load using the tm library\r\nlibrary(tm) \r\ndocs <- Corpus(DataframeSource(df.texts.clean))\r\n# Strip extra whitespace\r\ndocs <- tm_map(docs, stripWhitespace)\r\nbankA.idx = which(sapply(df$FullText,function(x) grepl(\"BankA\",x)))\r\nbankB.idx = which(sapply(df$FullText,function(x) grepl(\"BankB\",x)))\r\nbankC.idx = which(sapply(df$FullText,function(x) grepl(\"BankC\",x)))\r\nbankD.idx = which(sapply(df$FullText,function(x) grepl(\"BankD\",x)))\r\ndf.banka=df[bankA.idx,]\r\ndf.banka\r\nabroad.idx=which(sapply(df.banka$FullText,function(x) grepl(\"abroad\",x)))\r\nabroad.idx\r\nforeign.idx=which(sapply(df.banka$FullText,function(x) grepl(\"foreign\",x)))\r\nforeign.idx\r\ndf.final1=df.banka[abroad.idx,]\r\ndf.final2=df.banka[foreign.idx,]\r\ndf.final1\r\ndf.final2\r\ntotal_bankA <- rbind(df.final1,df.final2)\r\ndf.bankb=df[bankB.idx,]\r\ndf.bankb\r\nabroad.idx=which(sapply(df.bankb$FullText,function(x) grepl(\"abroad\",x)))\r\nabroad.idx\r\nforeign.idx=which(sapply(df.bankb$FullText,function(x) grepl(\"foreign\",x)))\r\nforeign.idx\r\ndf.final1=df.bankb[abroad.idx,]\r\ndf.final2=df.bankb[foreign.idx,]\r\ndf.final1\r\ndf.final2\r\ntotal_bankC <- rbind(df.final1,df.final2)\r\ndf.bankc=df[bankC.idx,]\r\ndf.bankc\r\nabroad.idx=which(sapply(df.bankc$FullText,function(x) grepl(\"abroad\",x)))\r\nabroad.idx\r\nforeign.idx=which(sapply(df.bankc$FullText,function(x) grepl(\"foreign\",x)))\r\nforeign.idx\r\ndf.final1=df.bankc[abroad.idx,]\r\ndf.final2=df.bankc[foreign.idx,]\r\ndf.final1\r\ndf.final2\r\ntotal_bankC <- rbind(df.final1,df.final2)\r\ndf.bankd=df[bankD.idx,]\r\ndf.bankd\r\nabroad.idx=which(sapply(df.bankd$FullText,function(x) grepl(\"abroad\",x)))\r\nabroad.idx\r\nforeign.idx=which(sapply(df.bankd$FullText,function(x) grepl(\"foreign\",x)))\r\nforeign.idx\r\ndf.final1=df.bankd[abroad.idx,]\r\ndf.final2=df.bankd[foreign.idx,]\r\ndf.final1\r\ndf.final2\r\ntotal_bankD <- rbind(df.final1,df.final2)\r\n# This turns out to be too slow\r\n# Add the metadata\r\n# This takes a bit to run\r\n# You can add more here\r\n‪#‎for‬‬‬ (i in 1:nrow(df)) {\r\n# meta(docs[[i]],\"MediaType\") = df$MediaType[i]\r\n# meta(docs[[i]],\"Year\") = df$Year[i]\r\n# if (grepl(\"BankA\",df$FullText[i])) {\r\n# meta(docs[[i]],\"BankA\") = T\r\n# } else {\r\n# meta(docs[[i]],\"BankA\") = F\r\n# }\r\n# if (grepl(\"BankB\",df$FullText[i])) {\r\n# meta(docs[[i]],\"BankB\") = T\r\n# } else {\r\n# meta(docs[[i]],\"BankB\") = F\r\n# }\r\n#}\r\n‪#‎bankA‬‬‬.idx <- meta(docs, \"BankA\") == T\r\nbankA.docs = docs[bankA.idx]\r\nbankB.docs = docs[bankB.idx]\r\nbankC.docs = docs[bankC.idx]\r\nbankD.docs = docs[bankD.idx]\r\nsummary(docs)\r\ndocs <- tm_map(docs, removePunctuation)  \r\n\r\nRunning the sentiment analysis:\r\n#-------Sentiment analysis Bank A-------\r\npos <- scan('positive-words.txt',what='character',comment.char=';')\r\nneg <- scan('negative-words.txt',what='character',comment.char=';')\r\nscore.sentiment = function(sentences, pos.words, neg.words, .progress='none')\r\n{\r\nrequire(plyr)\r\nrequire(stringr)\r\n# we got a vector of sentences. plyr will handle a list\r\n# or a vector as an \"l\" for us\r\n# we want a simple array (\"a\") of scores back, so we use \r\n# \"l\" + \"a\" + \"ply\" = \"laply\":\r\nscores = laply(sentences, function(sentence, pos.words, neg.words) {\r\n# clean up sentences with R's regex-driven global substitute, gsub():\r\nsentence = gsub('[[:punct:]]', '', sentence)\r\nsentence = gsub('[[:cntrl:]]', '', sentence)\r\nsentence = gsub('\\\\d+', '', sentence)\r\n# and convert to lower case:\r\nsentence = tolower(sentence)\r\n# split into words. str_split is in the stringr package\r\nword.list = str_split(sentence, '\\\\s+')\r\n# sometimes a list() is one level of hierarchy too much\r\nwords = unlist(word.list)\r\n# compare our words to the dictionaries of positive & negative terms\r\npos.matches = match(words, pos.words)\r\nneg.matches = match(words, neg.words)\r\n# match() returns the position of the matched term or NA\r\n# we just want a TRUE/FALSE:\r\npos.matches = !is.na(pos.matches)\r\nneg.matches = !is.na(neg.matches)\r\n# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():\r\nscore = sum(pos.matches) - sum(neg.matches)\r\nreturn(score)\r\n}, pos.words, neg.words, .progress=.progress )\r\nscores.df = data.frame(score=scores, text=sentences)\r\nreturn(scores.df)\r\n}\r\ntotal.sentiment = total_bankA\r\nscores = score.sentiment(as.character(total.sentiment$FullText), pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 1)\r\nscores$very.neg = as.numeric(scores$score <= -1)\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\nscores$mediatype = total.sentiment$MediaType\r\n# colors\r\ncols = c(\"‪#‎7CAE00‬‬‬\", \"‪#‎00BFC4‬‬‬\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\ngeom_boxplot(aes(fill=mediatype)) +\r\nscale_fill_manual(values=cols) +\r\ngeom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\nlabs(title = \"Media Type's Sentiment Scores\") + \r\nxlab('Media Type') + ylab('Sentiment Score')\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Sentiment Score\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mean_pos, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Positive Sentiment Score (Bank A)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_neg$mean_neg)\r\nggplot(mediatype_neg, aes(x = factor(mediatypes), y = mean_neg, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Negative Sentiment Score (Bank A)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n#-------Sentiment analysis Bank B-------\r\ntotal.sentiment = total_bankB\r\nscores = score.sentiment(as.character(total.sentiment$FullText), pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 1)\r\nscores$very.neg = as.numeric(scores$score <= -1)\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\nscores$mediatype = total.sentiment$MediaType\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\ngeom_boxplot(aes(fill=mediatype)) +\r\nscale_fill_manual(values=cols) +\r\ngeom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\nlabs(title = \"Media Type's Sentiment Scores\") + \r\nxlab('Media Type') + ylab('Sentiment Score')\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Sentiment Score\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mean_pos, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Positive Sentiment Score (Bank B)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_neg$mean_neg)\r\nggplot(mediatype_neg, aes(x = factor(mediatypes), y = mean_neg, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Negative Sentiment Score (Bank B)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n#-------Sentiment analysis Bank C-------\r\ntotal.sentiment = total_bankC\r\nscores = score.sentiment(as.character(total.sentiment$FullText), pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 1)\r\nscores$very.neg = as.numeric(scores$score <= -1)\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\nscores$mediatype = total.sentiment$MediaType\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\ngeom_boxplot(aes(fill=mediatype)) +\r\nscale_fill_manual(values=cols) +\r\ngeom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\nlabs(title = \"Media Type's Sentiment Scores\") + \r\nxlab('Media Type') + ylab('Sentiment Score')\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Sentiment Score\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mean_pos, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Positive Sentiment Score (Bank C)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_neg$mean_neg)\r\nggplot(mediatype_neg, aes(x = factor(mediatypes), y = mean_neg, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Negative Sentiment Score (Bank C)\") +\r\nxlab('Media Type') + ylab('Average Score')\r\n#-------Sentiment analysis Bank D-------\r\ntotal.sentiment = total_bankD\r\nscores = score.sentiment(as.character(total.sentiment$FullText), pos, neg, .progress='text')\r\nscores$very.pos = as.numeric(scores$score >= 1)\r\nscores$very.neg = as.numeric(scores$score <= -1)\r\n# how many very positives and very negatives\r\nnumpos = sum(scores$very.pos)\r\nnumneg = sum(scores$very.neg)\r\n# global score\r\nglobal_score = round( 100 * numpos / (numpos + numneg) )\r\n# colors\r\ncols = c(\"#7CAE00\", \"#00BFC4\")\r\nnames(cols) = c(\"twitter\", \"facebook\")\r\n# boxplot\r\nlibrary(ggplot2)\r\nggplot(scores, aes(x=mediatype, y=score, group=mediatype)) +\r\ngeom_boxplot(aes(fill=mediatype)) +\r\nscale_fill_manual(values=cols) +\r\ngeom_jitter(colour=\"gray40\",position=position_jitter(width=0.2), alpha=0.3) +\r\nlabs(title = \"Media Type's Sentiment Scores\") + \r\nxlab('Media Type') + ylab('Sentiment Score')\r\n# barplot of average score\r\nmeanscore = tapply(scores$score, scores$mediatype, mean)\r\ndf.plot = data.frame(mediatype=names(meanscore), meanscore=meanscore)\r\ndf.plot$mediatypes <- reorder(df.plot$mediatype, df.plot$meanscore)\r\nggplot(df.plot, aes(x = factor(mediatypes), y = meanscore, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Sentiment Score\") + \r\nxlab('Media Type') + ylab('Average Score')\r\n# barplot of average very positive\r\nmediatype_pos = ddply(scores, .(mediatype), summarise, mean_pos=mean(very.pos))\r\nmediatype_pos$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_pos$mean_pos)\r\nggplot(mediatype_pos, aes(x = factor(mediatypes), y = mean_pos, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Positive Sentiment Score (Bank D)\") + \r\nxlab('Media Type') + ylab('Average Score')\r\nmediatype_neg = ddply(scores, .(mediatype), summarise, mean_neg=mean(very.neg))\r\nmediatype_neg$mediatypes <- reorder(mediatype_pos$mediatype, mediatype_neg$mean_neg)\r\nggplot(mediatype_neg, aes(x = factor(mediatypes), y = mean_neg, fill=mediatypes)) +\r\ngeom_bar(stat=\"identity\") +\r\nscale_fill_manual(values=cols[order(df.plot$meanscore)]) +\r\nlabs(title = \"Average Fraction Very Negative Sentiment Score (Bank D)\") +\r\nxlab('Media Type') + ylab('Average Score')\r\n```\r\n\r\n### Graphs for Sentiment Analysis\r\n\r\nhttp://i.imgur.com/4bGuauq.gif\r\n\r\n\r\nBank A had an average of 0.32:0.28 (1.14) (twitter) and 0.38:0.32 (1.19)(Facebook) (Negative: Positive)  \r\nBank B had an average 0.24:0.27 (0.89) (twitter) and 0.38:0.21 (1.81) (Facebook) (Negative: Positive)\r\nBank C had an average ratio (Negative: Positive) of 0.46:0.16 (2.88) (twitter) and 0.34:0.24 (1.42) (Facebook) \r\nBank D had an average ratio (Negative: Positive) of 0.16:0.07 (2.29) (twitter) and 0.46:0.26 (1.77) (Facebook)\r\n\r\n\r\n## Conclusion\r\n\r\nAfter analyzing the ratios from our sentiment we found that for the topics of foreign and abroad Bank C had the most negative posts with Bank D as a close second. We further investigated the data to determine the substances and found that customers were unhappy with bank cards not working abroad. We also found that many customers felt negatively towards foreign exchange fees, some claiming that Bank D and Bank C must be “rigging” the fee rates. Bank A and Bank B had fairly even negative and positive ratios, additionally they had the least percentage of negative posts of the 4 banks when considering the topics of foreign and abroad transactions. We don’t believe we can necessarily draw any definite inferences from the sentiment analysis as there is no clear evidence that the banks did in fact rig the foreign exchange fee rates, but we can see that it is an important factor that customers are associating with these banks. The use of cards abroad is also an important factor to these customers, and can be an issue that these banks may find worthwhile investigating further. \r\n\r\n## New Content\r\n\r\nI took our code and expanded upon it to understand what words in general are most frequently being discussed. So I implemented a code that created a plot of word frequency and a word cloud. Below is the code:\r\n\r\n```R\r\nWord Frequency plot for words occurring 400 or more times\r\ninspect(tdm[idx + (0:5), 101:110])\r\n(freq.terms <- findFreqTerms(tdm, lowfreq = 400))\r\nterm.freq <- rowSums(as.matrix(tdm))\r\nterm.freq <- subset(term.freq, term.freq >= 400)\r\ndf <- data.frame(term = names(term.freq), freq = term.freq)\r\nlibrary(ggplot2)\r\nggplot(df, aes(x = term, y = freq)) + geom_bar(stat = \"identity\") +\r\n  xlab(\"Terms\") + ylab(\"Count\") + coord_flip()\r\n\r\nWordcloud for words occurring 500 or more times\r\nset.seed(142)   \r\ndark2 <- brewer.pal(6, \"Dark2\")   \r\nwordcloud(names(freq), freq, max.words=500, rot.per=0.2, colors=dark2)   \r\n```\r\nhttp://i.imgur.com/pewJolq.png\r\n\r\nhttp://i.imgur.com/GnCIybL.jpg\r\n\r\n#### Authors and Contributors\r\n@koatliky - Major: Data Science\r\n@mikaelaray - Major: Sociology Minor: Data Science \r\n\r\n#### Sources and Inspiration\r\nhttp://www.rdatamining.com/docs/text-mining-with-r-of-twitter-data-analysis\r\nhttps://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html#term-correlations\r\n\r\nMinqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \r\n       Proceedings of the ACM SIGKDD International Conference on Knowledge \r\n       Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \r\n       Washington, USA, \r\n   Bing Liu, Minqing Hu and Junsheng Cheng. \"Opinion Observer: Analyzing \r\n       and Comparing Opinions on the Web.\" Proceedings of the 14th \r\n       International World Wide Web conference (WWW-2005), May 10-14, \r\n       2005, Chiba, Japan.\r\nSome code based on http://www.ihub.co.ke/blogs/23216 and modified and provided by Dr. Paul Anderson, College of Charleston\r\n\r\n##### Support or Contact\r\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/pages) or [contact support](https://github.com/contact) and we’ll help you sort it out.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}